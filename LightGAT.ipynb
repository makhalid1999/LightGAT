{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1foyAlNh01oQKh3vcpnsPiGgmath1Fxpe",
      "authorship_tag": "ABX9TyMh2Svok5tm08k3CvAXlHfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makhalid1999/LightGAT/blob/main/LightGAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vWtjpyt686Z",
        "outputId": "1e139581-8875-4619-cd32-92d18b8652b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=f55b5827aed73179855c61785d9d1248c7649aa2516e80f2261224c21b6ba340\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "dataset = KarateClub()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feE04049v1m7",
        "outputId": "c038b833-620b-45c0-f901-ce250af902fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: KarateClub():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 34\n",
            "Number of classes: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.parameter as Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LightGAT_Layer(nn.Module):\n",
        "  def __init__(self, num_nodes, num_features):\n",
        "    self.num_nodes = num_nodes\n",
        "    self.num_features = num_features\n",
        "    super(LightGAT_Layer, self).__init__()\n",
        "    self.a = Parameter.Parameter(torch.randn(1, num_features*2))\n",
        "    self.a.requires_grad = True\n",
        "  def forward(self, x, adj):\n",
        "    attention = torch.zeros(self.num_nodes, self.num_nodes)\n",
        "    for i in range(self.num_nodes):\n",
        "      for j in range(self.num_nodes):\n",
        "        if adj[i, j] != 0:\n",
        "          b = torch.cat((x[i], x[j]))\n",
        "          attention[i, j] = torch.mm(self.a, torch.transpose(b.unsqueeze(0), 1, 0))\n",
        "    e = nn.functional.softmax(attention, dim = 1)\n",
        "    e_adj = e * adj\n",
        "    return torch.sparse.mm(e_adj, x)\n",
        "\n",
        "class LightGAT_Layer2(nn.Module):\n",
        "  def __init__(self, num_nodes, num_features):\n",
        "    super(LightGAT_Layer2, self).__init__()\n",
        "    self.num_nodes = num_nodes\n",
        "    self.num_features = num_features\n",
        "    self.a1 = Parameter.Parameter(torch.randn(num_features, num_features))\n",
        "    self.a1.requires_grad = True\n",
        "    self.a2 = Parameter.Parameter(torch.randn(num_features, num_features))\n",
        "    self.a2.requires_grad = True\n",
        "  def forward(self, x, adj):\n",
        "    a1_x = torch.mm(self.a1, x)\n",
        "    a2_x = torch.mm(self.a2, x)\n",
        "    attention = torch.mm(a1_x, torch.transpose(a2_x, 1, 0))\n",
        "    e = nn.functional.softmax(attention, dim = 1)\n",
        "    e_adj = e * adj\n",
        "    return torch.sparse.mm(e_adj, x)\n",
        "\n",
        "class LightGCN_Layer(nn.Module):\n",
        "  def __init__(self, num_nodes):\n",
        "    super(LightGCN_Layer, self).__init__()\n",
        "  def forward(self, x, adj):\n",
        "    return torch.sparse.mm(adj, x)\n",
        "\n",
        "class GCN_Layer(nn.Module):\n",
        "  def __init__(self, num_nodes):\n",
        "    super(GCN_Layer, self).__init__()\n",
        "    self.W = Parameter.Parameter(torch.eye(num_nodes, num_nodes))\n",
        "    self.W.requires_grad = True\n",
        "  def forward(self, x, adj):\n",
        "    x1 = torch.mm(x, self.W)\n",
        "    return torch.sparse.mm(adj, x1)\n",
        "\n",
        "class GAT_Layer(nn.Module):\n",
        "  def __init__(self, num_nodes, num_features):\n",
        "    super(GAT_Layer, self).__init__()\n",
        "    self.num_nodes = num_nodes\n",
        "    self.num_features = num_features\n",
        "    self.a = Parameter.Parameter(torch.randn(1, num_features*2))\n",
        "    self.a.requires_grad = True\n",
        "    self.W = Parameter.Parameter(torch.eye(num_nodes, num_nodes))\n",
        "    self.W.requires_grad = True  \n",
        "  def forward(self, x, adj):\n",
        "    x1 = torch.mm(x, self.W)    \n",
        "    attention = torch.zeros(self.num_nodes, self.num_nodes)\n",
        "    for i in range(self.num_nodes):\n",
        "      for j in range(self.num_nodes):\n",
        "        if adj[i, j] != 0:\n",
        "          b = torch.cat((x1[i], x1[j]))\n",
        "          attention[i, j] = torch.mm(self.a, torch.transpose(b.unsqueeze(0), 1, 0))\n",
        "    e = nn.functional.softmax(attention, dim = 1)\n",
        "    e_adj = e * adj\n",
        "    return torch.sparse.mm(e_adj, x1) \n",
        "\n",
        "class GAT_Layer2(nn.Module):\n",
        "  def __init__(self, num_nodes, num_features):\n",
        "    super(GAT_Layer2, self).__init__()\n",
        "    self.num_nodes = num_nodes\n",
        "    self.num_features = num_features\n",
        "    self.a1 = Parameter.Parameter(torch.randn(num_features, num_features))\n",
        "    self.a1.requires_grad = True\n",
        "    self.a2 = Parameter.Parameter(torch.randn(num_features, num_features))\n",
        "    self.a2.requires_grad = True\n",
        "    self.W = Parameter.Parameter(torch.eye(num_nodes, num_nodes))\n",
        "    self.W.requires_grad = True  \n",
        "  def forward(self, x, adj):\n",
        "    a1_x = torch.mm(self.a1, x)\n",
        "    a2_x = torch.mm(self.a2, x)\n",
        "    attention = torch.mm(a1_x, torch.transpose(a2_x, 1, 0))\n",
        "    e = nn.functional.softmax(attention, dim = 1)\n",
        "    e_adj = e * adj\n",
        "    x1 = torch.mm(x, self.W)\n",
        "    return torch.sparse.mm(e_adj, x1)"
      ],
      "metadata": {
        "id": "_ga03hT09804"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGAT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LightGAT, self).__init__()\n",
        "    self.L1 = LightGAT_Layer(34, 34)\n",
        "    self.L2 = LightGAT_Layer(34, 34)\n",
        "    self.classifier = nn.Linear(34, 4)\n",
        "    self.x = Parameter.Parameter(torch.eye(34, 34))\n",
        "    self.x.requires_grad = True\n",
        "  def forward(self, adj):\n",
        "    h1 = self.L1(self.x, adj)\n",
        "    h2 = self.L2(h1, adj)\n",
        "    out = F.softmax(self.classifier(h2), dim = 1)\n",
        "    return out\n",
        "\n",
        "class LightGAT2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LightGAT2, self).__init__()\n",
        "    self.L1 = LightGAT_Layer2(34, 34)\n",
        "    self.L2 = LightGAT_Layer2(34, 34)\n",
        "    self.classifier = nn.Linear(34, 4)\n",
        "    self.x = Parameter.Parameter(torch.eye(34, 34))\n",
        "    self.x.requires_grad = True\n",
        "  def forward(self, adj):\n",
        "    h1 = self.L1(self.x, adj)\n",
        "    h2 = self.L2(h1, adj)\n",
        "    out = F.softmax(self.classifier(h2), dim = 1)\n",
        "    return out\n",
        "\n",
        "class LightGCN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LightGCN, self).__init__()\n",
        "    self.L1 = LightGCN_Layer(34)\n",
        "    self.L2 = LightGCN_Layer(34)\n",
        "    self.classifier = nn.Linear(34, 4)\n",
        "    self.x = Parameter.Parameter(torch.eye(34, 34))\n",
        "    self.x.requires_grad = True\n",
        "  def forward(self, adj):\n",
        "    h1 = self.L1(self.x, adj)\n",
        "    h2 = self.L2(h1, adj)\n",
        "    out = F.softmax(self.classifier(h2), dim = 1)\n",
        "    return out\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GCN, self).__init__()\n",
        "    self.L1 = GCN_Layer(34)\n",
        "    self.L2 = GCN_Layer(34)\n",
        "    self.classifier = nn.Linear(34, 4)\n",
        "    self.x = Parameter.Parameter(torch.eye(34, 34))\n",
        "    self.x.requires_grad = True\n",
        "  def forward(self, adj):\n",
        "    h1 = self.L1(self.x, adj)\n",
        "    h1.tanh()\n",
        "    h2 = self.L2(h1, adj)\n",
        "    h2.tanh()\n",
        "    out = F.softmax(self.classifier(h2), dim = 1)\n",
        "    return out\n",
        "\n",
        "class GAT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GAT, self).__init__()\n",
        "    self.L1 = GAT_Layer(34, 34)\n",
        "    self.L2 = GAT_Layer(34, 34)\n",
        "    self.classifier = nn.Linear(34, 4)\n",
        "    self.x = Parameter.Parameter(torch.eye(34, 34))\n",
        "    self.x.requires_grad = True\n",
        "  def forward(self, adj):\n",
        "    h1 = self.L1(self.x, adj)\n",
        "    h1.tanh()\n",
        "    h2 = self.L2(h1, adj)\n",
        "    h2.tanh()\n",
        "    out = F.softmax(self.classifier(h2), dim = 1)\n",
        "    return out\n",
        "\n",
        "class GAT2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GAT2, self).__init__()\n",
        "    self.L1 = GAT_Layer2(34, 34)\n",
        "    self.L2 = GAT_Layer2(34, 34)\n",
        "    self.classifier = nn.Linear(34, 4)\n",
        "    self.x = Parameter.Parameter(torch.eye(34, 34))\n",
        "    self.x.requires_grad = True\n",
        "  def forward(self, adj):\n",
        "    h1 = self.L1(self.x, adj)\n",
        "    h1.tanh()\n",
        "    h2 = self.L2(h1, adj)\n",
        "    h2.tanh()\n",
        "    out = F.softmax(self.classifier(h2), dim = 1)\n",
        "    return out"
      ],
      "metadata": {
        "id": "zzu4QouDVSsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "def adj(data):\n",
        "  a = torch.zeros(34, 34)\n",
        "  for i in data.edge_index.T:\n",
        "    a[i.tolist()[0],i.tolist()[1]] = 1\n",
        "  return a\n",
        "\n",
        "model = LightGAT()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj_m = adj(data)/torch.transpose(adj(data).sum(dim = 0).unsqueeze(0), 1, 0)\n",
        "    out = model(adj_m)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "    \n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "for epoch in range(100):\n",
        "    start = time.time()\n",
        "    loss, accuracy = train(data)\n",
        "    print('Loss: ' + str(loss.item()) + ' Training Accuracy: ' + str(accuracy['train'].item()) + ' Validation Accuracy: ' + str(accuracy['val'].item()))\n",
        "\n",
        "print('Total Time: ' + str(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ba_21fMUddS",
        "outputId": "7695f2a8-8a44-41f7-cf7e-46a5ecb17c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.386607050895691 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3865458965301514 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3864940404891968 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3864490985870361 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.386408805847168 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3863710165023804 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3863340616226196 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3862966299057007 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3862571716308594 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.386213779449463 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.386163592338562 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3861039876937866 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.386031985282898 Training Accuracy: 0.25 Validation Accuracy: 0.3529411852359772\n",
            "Loss: 1.3859457969665527 Training Accuracy: 0.25 Validation Accuracy: 0.3529411852359772\n",
            "Loss: 1.3858450651168823 Training Accuracy: 0.25 Validation Accuracy: 0.3529411852359772\n",
            "Loss: 1.3857293128967285 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3855959177017212 Training Accuracy: 0.5 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.3854413032531738 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.3852601051330566 Training Accuracy: 0.5 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.3850492238998413 Training Accuracy: 0.5 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.3848071098327637 Training Accuracy: 0.5 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.3845314979553223 Training Accuracy: 0.5 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.384217619895935 Training Accuracy: 0.5 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.3838567733764648 Training Accuracy: 0.5 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.3834378719329834 Training Accuracy: 0.5 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.3829460144042969 Training Accuracy: 0.5 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.3823591470718384 Training Accuracy: 0.5 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.3816410303115845 Training Accuracy: 0.75 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.3807311058044434 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.3795299530029297 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.3779051303863525 Training Accuracy: 0.5 Validation Accuracy: 0.29411765933036804\n",
            "Loss: 1.3757448196411133 Training Accuracy: 0.5 Validation Accuracy: 0.29411765933036804\n",
            "Loss: 1.3730555772781372 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.3699767589569092 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.3666212558746338 Training Accuracy: 0.5 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.362980842590332 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3590130805969238 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3547024726867676 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3500611782073975 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3451117277145386 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3398759365081787 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3343710899353027 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3286104202270508 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3226048946380615 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.316359281539917 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3098688125610352 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3031184673309326 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.2960869073867798 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.2887532711029053 Training Accuracy: 0.75 Validation Accuracy: 0.29411765933036804\n",
            "Loss: 1.2811022996902466 Training Accuracy: 0.75 Validation Accuracy: 0.3235294222831726\n",
            "Loss: 1.2731256484985352 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.2648229598999023 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.2561981678009033 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.2472593784332275 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.2380156517028809 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.2284762859344482 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.218649983406067 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.2085366249084473 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1981070041656494 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1872878074645996 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1760907173156738 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.1649553775787354 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.1544581651687622 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1445329189300537 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1349213123321533 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1255230903625488 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.116310715675354 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.1072742938995361 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.0984069108963013 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.0897026062011719 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.0811541080474854 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.0727486610412598 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.0644704103469849 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.0563035011291504 Training Accuracy: 1.0 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.0482341051101685 Training Accuracy: 1.0 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.0402491092681885 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.0323346853256226 Training Accuracy: 1.0 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.0244755744934082 Training Accuracy: 1.0 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.0166596174240112 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.0088927745819092 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.0011999607086182 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.9935820698738098 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.9860274195671082 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.9785330891609192 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.971100926399231 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9637343287467957 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9564379453659058 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9492170810699463 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9420781135559082 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9350280165672302 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9280754327774048 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9212298393249512 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9145017862319946 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9079020023345947 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9014413356781006 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8951300382614136 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.8889779448509216 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.8829941749572754 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.8771862983703613 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.8715609908103943 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Total Time: 0.13358163833618164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "def adj(data):\n",
        "  a = torch.zeros(34, 34)\n",
        "  for i in data.edge_index.T:\n",
        "    a[i.tolist()[0],i.tolist()[1]] = 1\n",
        "  return a\n",
        "\n",
        "model = LightGAT2()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj_m = adj(data)/torch.transpose(adj(data).sum(dim = 0).unsqueeze(0), 1, 0)\n",
        "    out = model(adj_m)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "    \n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "for epoch in range(100):\n",
        "    start = time.time()\n",
        "    loss, accuracy = train(data)\n",
        "    print('Loss: ' + str(loss.item()) + ' Training Accuracy: ' + str(accuracy['train'].item()) + ' Validation Accuracy: ' + str(accuracy['val'].item()))\n",
        "\n",
        "print('Total Time: ' + str(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KFWSVtVNMd4",
        "outputId": "bc1506ae-0a4b-40b5-e9ab-0c8a6937bc7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3864359855651855 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.386380672454834 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3863245248794556 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.386281967163086 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.386240005493164 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3861724138259888 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3860726356506348 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3858708143234253 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3856756687164307 Training Accuracy: 0.25 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.3854405879974365 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.385114073753357 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.3846750259399414 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.384122610092163 Training Accuracy: 0.5 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.383483648300171 Training Accuracy: 0.5 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3828074932098389 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.3821316957473755 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.3814480304718018 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.3807482719421387 Training Accuracy: 0.5 Validation Accuracy: 0.3529411852359772\n",
            "Loss: 1.3800349235534668 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3792638778686523 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3784260749816895 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3775111436843872 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3765125274658203 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3754206895828247 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.374223232269287 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.372909426689148 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3714548349380493 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.369850993156433 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.368084192276001 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3661551475524902 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3640803098678589 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3618676662445068 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3595030307769775 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3569657802581787 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3542506694793701 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3513492345809937 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3482656478881836 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3448967933654785 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3412991762161255 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3374154567718506 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3331727981567383 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3285024166107178 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3231992721557617 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.318023681640625 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3131952285766602 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3088047504425049 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.3042011260986328 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2993812561035156 Training Accuracy: 0.25 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2943384647369385 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.289292573928833 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.284279704093933 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2792346477508545 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2741096019744873 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2698662281036377 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2637076377868652 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2582550048828125 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2527660131454468 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2546443939208984 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2428991794586182 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2484564781188965 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2346080541610718 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2308145761489868 Training Accuracy: 0.5 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.2270010709762573 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2231451272964478 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2192556858062744 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2153440713882446 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2114191055297852 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.2074871063232422 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.203550934791565 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.199610710144043 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1956645250320435 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.191709280014038 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1877412796020508 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1837584972381592 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1797597408294678 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1757457256317139 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.171717882156372 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1676790714263916 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1636327505111694 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.159582257270813 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1555312871932983 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1514832973480225 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1474416255950928 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.143409252166748 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1393890380859375 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.135384202003479 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1313973665237427 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1274312734603882 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1234885454177856 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1195719242095947 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1156840324401855 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1118271350860596 Training Accuracy: 0.5 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 1.1080033779144287 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.104215145111084 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.1004642248153687 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.096752405166626 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.093080997467041 Training Accuracy: 1.0 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.089451551437378 Training Accuracy: 1.0 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0858650207519531 Training Accuracy: 1.0 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.082322359085083 Training Accuracy: 1.0 Validation Accuracy: 0.47058823704719543\n",
            "Total Time: 0.006026268005371094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "def adj(data):\n",
        "  a = torch.zeros(34, 34)\n",
        "  for i in data.edge_index.T:\n",
        "    a[i.tolist()[0],i.tolist()[1]] = 1\n",
        "  return a\n",
        "\n",
        "model = LightGCN()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.    \n",
        "    adj_m = adj(data)/torch.transpose(adj(data).sum(dim = 0).unsqueeze(0), 1, 0)\n",
        "    out = model(adj_m)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "    \n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "for epoch in range(100):\n",
        "    start = time.time()\n",
        "    loss, accuracy = train(data)\n",
        "    print('Loss: ' + str(loss.item()) + ' Training Accuracy: ' + str(accuracy['train'].item()) + ' Validation Accuracy: ' + str(accuracy['val'].item()))\n",
        "\n",
        "print('Total Time: ' + str(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSibmKZ704Zh",
        "outputId": "e2046630-aa96-4ee7-a3b9-db822bab5488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.384352445602417 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3784607648849487 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3724428415298462 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.3660521507263184 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 1.3590610027313232 Training Accuracy: 0.75 Validation Accuracy: 0.3529411852359772\n",
            "Loss: 1.3513187170028687 Training Accuracy: 0.75 Validation Accuracy: 0.529411792755127\n",
            "Loss: 1.3427104949951172 Training Accuracy: 1.0 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.3331363201141357 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.3225035667419434 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.3107249736785889 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 1.2977209091186523 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 1.283425211906433 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 1.2677892446517944 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 1.2507901191711426 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 1.2324376106262207 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 1.2127817869186401 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.1919150352478027 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.1699721813201904 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.1471197605133057 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.1235421895980835 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.0994292497634888 Training Accuracy: 1.0 Validation Accuracy: 0.5588235259056091\n",
            "Loss: 1.0749785900115967 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.0503945350646973 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.025883436203003 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 1.0016520023345947 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.9779098033905029 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.9548697471618652 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.9327482581138611 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.911758303642273 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8920977711677551 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8739330172538757 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8573811650276184 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8424989581108093 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8292787671089172 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.8176567554473877 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.8075270652770996 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7987590432167053 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7912119030952454 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7847444415092468 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7792217135429382 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7745182514190674 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7705199718475342 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7671248912811279 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7642427682876587 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7617951035499573 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.759713888168335 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7579410672187805 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.756427526473999 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.755131721496582 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7540187239646912 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7530595660209656 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7522299885749817 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7515098452568054 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7508823871612549 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7503334879875183 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7498517036437988 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.749427080154419 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.749051570892334 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7487184405326843 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7484215497970581 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7481564283370972 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7479187250137329 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7477049231529236 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7475121021270752 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7473375797271729 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7471791505813599 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.747035026550293 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7469034194946289 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7467830777168274 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7466726303100586 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7465710043907166 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7464771866798401 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7463904619216919 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7463099956512451 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7462353110313416 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7461656928062439 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.746100664138794 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7460398077964783 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7459827065467834 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7459290027618408 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7458784580230713 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.745830774307251 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7457855939865112 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7457427978515625 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7457020878791809 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7456635236740112 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7456266283988953 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7455914616584778 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7455577850341797 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7455254793167114 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7454946637153625 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7454649209976196 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7454363107681274 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7454087734222412 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7453821897506714 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7453565001487732 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7453316450119019 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7453076243400574 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.745284378528595 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7452618479728699 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Total Time: 0.00513458251953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "def adj(data):\n",
        "  a = torch.zeros(34, 34)\n",
        "  for i in data.edge_index.T:\n",
        "    a[i.tolist()[0],i.tolist()[1]] = 1\n",
        "  return a\n",
        "\n",
        "model = GCN()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj_m = adj(data)/torch.transpose(adj(data).sum(dim = 0).unsqueeze(0), 1, 0)\n",
        "    out = model(adj_m)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "    \n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "for epoch in range(100):\n",
        "    start = time.time()\n",
        "    loss, accuracy = train(data)\n",
        "    print('Loss: ' + str(loss.item()) + ' Training Accuracy: ' + str(accuracy['train'].item()) + ' Validation Accuracy: ' + str(accuracy['val'].item()))\n",
        "\n",
        "print('Total Time: ' + str(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x4PsbXL06yW",
        "outputId": "ffe16812-0ce6-4cb2-b62c-51c70232aa94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3955817222595215 Training Accuracy: 0.25 Validation Accuracy: 0.29411765933036804\n",
            "Loss: 1.364160418510437 Training Accuracy: 1.0 Validation Accuracy: 0.7058823704719543\n",
            "Loss: 1.3065834045410156 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 1.1752618551254272 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.9786765575408936 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.8519307971000671 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.7763817310333252 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.7468534708023071 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.7444478273391724 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.7440365552902222 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.7438129782676697 Training Accuracy: 1.0 Validation Accuracy: 0.5882353186607361\n",
            "Loss: 0.7437161803245544 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436828017234802 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436726689338684 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7436695694923401 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7436687350273132 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7436684966087341 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7436683773994446 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Total Time: 0.005250215530395508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "def adj(data):\n",
        "  a = torch.zeros(34, 34)\n",
        "  for i in data.edge_index.T:\n",
        "    a[i.tolist()[0],i.tolist()[1]] = 1\n",
        "  return a\n",
        "\n",
        "model = GAT()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj_m = adj(data)/torch.transpose(adj(data).sum(dim = 0).unsqueeze(0), 1, 0)\n",
        "    out = model(adj_m)  # Perform a single forward pass.  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "    \n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "for epoch in range(100):\n",
        "    start = time.time()\n",
        "    loss, accuracy = train(data)\n",
        "    print('Loss: ' + str(loss.item()) + ' Training Accuracy: ' + str(accuracy['train'].item()) + ' Validation Accuracy: ' + str(accuracy['val'].item()))\n",
        "\n",
        "print('Total Time: ' + str(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahnh-2gM09Hd",
        "outputId": "bc7021fa-3f5f-43b6-dbd0-d8fa60ba78f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3866838216781616 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.38661527633667 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3865504264831543 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3864741325378418 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.386365294456482 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3861886262893677 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.385871171951294 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3852218389511108 Training Accuracy: 0.25 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.3837295770645142 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.3803622722625732 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.3739759922027588 Training Accuracy: 0.5 Validation Accuracy: 0.2647058963775635\n",
            "Loss: 1.3645603656768799 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.352084994316101 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.3345215320587158 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.3128941059112549 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.2892757654190063 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.2641698122024536 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.24033522605896 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.2202831506729126 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.2035672664642334 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.1867479085922241 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.1675255298614502 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.148195505142212 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.1276546716690063 Training Accuracy: 0.5 Validation Accuracy: 0.2647058963775635\n",
            "Loss: 1.107728123664856 Training Accuracy: 0.5 Validation Accuracy: 0.2647058963775635\n",
            "Loss: 1.0925110578536987 Training Accuracy: 0.5 Validation Accuracy: 0.2647058963775635\n",
            "Loss: 1.0837727785110474 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.0799912214279175 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.0756069421768188 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.06631338596344 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.057675838470459 Training Accuracy: 0.5 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.0464122295379639 Training Accuracy: 0.5 Validation Accuracy: 0.14705882966518402\n",
            "Loss: 1.0314313173294067 Training Accuracy: 0.75 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.0134048461914062 Training Accuracy: 0.75 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 0.9950774908065796 Training Accuracy: 0.75 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 0.9783495664596558 Training Accuracy: 0.75 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 0.9638388752937317 Training Accuracy: 0.75 Validation Accuracy: 0.38235294818878174\n",
            "Loss: 0.9510109424591064 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 0.9386135339736938 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 0.9265332221984863 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 0.9209089279174805 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9110013246536255 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 0.9079609513282776 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 0.9213843941688538 Training Accuracy: 0.75 Validation Accuracy: 0.4117647111415863\n",
            "Loss: 0.9021300077438354 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.899747371673584 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 0.9100736379623413 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 0.8964439630508423 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8957744240760803 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 0.8944696187973022 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 0.892741322517395 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 0.8906395435333252 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8881969451904297 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.885444164276123 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8824095726013184 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.879118800163269 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8755925893783569 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8718470931053162 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8678929209709167 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8637356162071228 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8593758344650269 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8548100590705872 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8500335812568665 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8450453281402588 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8398610353469849 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8345391750335693 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.829237699508667 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8243169188499451 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8204358816146851 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.818162202835083 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8162927627563477 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8124843835830688 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8074978590011597 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8033585548400879 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8003997802734375 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.7978354692459106 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.7949649095535278 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.7915292978286743 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.7876311540603638 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.7835733890533447 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7797151803970337 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7763352990150452 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7734723687171936 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7708314657211304 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.76806640625 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7652199268341064 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7625795602798462 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7602898478507996 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7583180069923401 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7565856575965881 Training Accuracy: 1.0 Validation Accuracy: 0.6176470518112183\n",
            "Loss: 0.7550357580184937 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7536391615867615 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7523835897445679 Training Accuracy: 1.0 Validation Accuracy: 0.6470588445663452\n",
            "Loss: 0.7512638568878174 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7502734661102295 Training Accuracy: 1.0 Validation Accuracy: 0.7058823704719543\n",
            "Loss: 0.7494014501571655 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7486366033554077 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7479737997055054 Training Accuracy: 1.0 Validation Accuracy: 0.6764705777168274\n",
            "Loss: 0.7474063634872437 Training Accuracy: 1.0 Validation Accuracy: 0.7058823704719543\n",
            "Loss: 0.7469235062599182 Training Accuracy: 1.0 Validation Accuracy: 0.7058823704719543\n",
            "Total Time: 0.0868523120880127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "def adj(data):\n",
        "  a = torch.zeros(34, 34)\n",
        "  for i in data.edge_index.T:\n",
        "    a[i.tolist()[0],i.tolist()[1]] = 1\n",
        "  return a\n",
        "\n",
        "model = GAT2()\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj_m = adj(data)/torch.transpose(adj(data).sum(dim = 0).unsqueeze(0), 1, 0)\n",
        "    out = model(adj_m)  # Perform a single forward pass.  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "    accuracy = {}\n",
        "    # Calculate training accuracy on our four examples\n",
        "    predicted_classes = torch.argmax(out[data.train_mask], axis=1) # [0.6, 0.2, 0.7, 0.1] -> 2\n",
        "    target_classes = data.y[data.train_mask]\n",
        "    accuracy['train'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "    \n",
        "    # Calculate validation accuracy on the whole graph\n",
        "    predicted_classes = torch.argmax(out, axis=1)\n",
        "    target_classes = data.y\n",
        "    accuracy['val'] = torch.mean(\n",
        "        torch.where(predicted_classes == target_classes, 1, 0).float())\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "for epoch in range(100):\n",
        "    start = time.time()\n",
        "    loss, accuracy = train(data)\n",
        "    print('Loss: ' + str(loss.item()) + ' Training Accuracy: ' + str(accuracy['train'].item()) + ' Validation Accuracy: ' + str(accuracy['val'].item()))\n",
        "\n",
        "print('Total Time: ' + str(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvB7CBHUPy6",
        "outputId": "c5669dcd-9e99-4476-aaaf-6d4233bb2791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.3863977193832397 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3862807750701904 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3861726522445679 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3860305547714233 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3858325481414795 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3855502605438232 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3851741552352905 Training Accuracy: 0.25 Validation Accuracy: 0.11764705926179886\n",
            "Loss: 1.3845819234848022 Training Accuracy: 0.25 Validation Accuracy: 0.1764705926179886\n",
            "Loss: 1.3827365636825562 Training Accuracy: 0.75 Validation Accuracy: 0.5\n",
            "Loss: 1.3800532817840576 Training Accuracy: 0.5 Validation Accuracy: 0.23529411852359772\n",
            "Loss: 1.376124382019043 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.3694018125534058 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.355973243713379 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.3376190662384033 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.30868661403656 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.2782831192016602 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.229113221168518 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.1919556856155396 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.1452805995941162 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.119489073753357 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.10101318359375 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0889559984207153 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.081762433052063 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0776832103729248 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.075218677520752 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0734739303588867 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0720078945159912 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0706266164779663 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0692514181137085 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0678541660308838 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.066429615020752 Training Accuracy: 0.5 Validation Accuracy: 0.20588235557079315\n",
            "Loss: 1.0649800300598145 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.063509225845337 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.0620229244232178 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.060524582862854 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0590181350708008 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0575069189071655 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0559942722320557 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0544816255569458 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0529725551605225 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0514682531356812 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0499687194824219 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.048476219177246 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0469914674758911 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0455169677734375 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0440787076950073 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0426054000854492 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0411280393600464 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0397194623947144 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0382306575775146 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.03676176071167 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.035335898399353 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0338233709335327 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0322117805480957 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0305639505386353 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0288591384887695 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0271803140640259 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0249996185302734 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.022810459136963 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.020628571510315 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0183857679367065 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0160576105117798 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0135085582733154 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0106477737426758 Training Accuracy: 0.75 Validation Accuracy: 0.47058823704719543\n",
            "Loss: 1.0073930025100708 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 1.003647804260254 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9992984533309937 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9942119121551514 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9882396459579468 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9812275171279907 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9730375409126282 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9636083841323853 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9536591172218323 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9421619176864624 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9307578802108765 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9192488193511963 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.9083630442619324 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.8988239169120789 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.8911014795303345 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.8852707147598267 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.8810638189315796 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.8780527710914612 Training Accuracy: 0.75 Validation Accuracy: 0.44117647409439087\n",
            "Loss: 0.8758211731910706 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8740495443344116 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8725266456604004 Training Accuracy: 1.0 Validation Accuracy: 0.5\n",
            "Loss: 0.8711252808570862 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8697741031646729 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8684353232383728 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.867090106010437 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8657304644584656 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8643532395362854 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8629587888717651 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8615480661392212 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8601236939430237 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8586874008178711 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8572415113449097 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8557880520820618 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8543288707733154 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8528658151626587 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Loss: 0.8514005541801453 Training Accuracy: 1.0 Validation Accuracy: 0.529411792755127\n",
            "Total Time: 0.007670879364013672\n"
          ]
        }
      ]
    }
  ]
}